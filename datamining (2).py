# -*- coding: utf-8 -*-
"""datamining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmPwkwnvlI9G2i5zUVhCMGBFgyST3pAE
"""

!nvidia-smi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
import warnings
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
warnings.filterwarnings("ignore")

# Step 1: Load your accident data CSV
df = pd.read_csv("/content/AccidentLondonBoroughs2223.csv")

# Step 2: Load metadata XLS file with all mappings
meta = pd.read_excel("/content/Road-Accident-Safety-Data-Guide.xls", sheet_name=None)

# Step 3: Utility function to convert code-label sheets into dictionaries
def load_meta(sheet_df):
    sheet_df.columns = [c.lower().strip() for c in sheet_df.columns]
    return dict(zip(sheet_df['code'], sheet_df['label']))

# Step 4: Prepare mappings (match column names in CSV)
mappings = {
    "Police_Force": "Police Force",
    "Accident_Severity": "Accident Severity",
    "Day_of_Week": "Day of Week",
    "Road_Type": "Road Type",
    "Weather_Conditions": "Weather",
    "Road_Surface_Conditions": "Road Surface",
    "Light_Conditions": "Light Conditions"
}

# Step 5: Apply mappings and create new label columns
for col, sheet in mappings.items():
    if col in df.columns and sheet in meta:
        mapping_dict = load_meta(meta[sheet])
        df[col + "_Label"] = df[col].map(mapping_dict)

# Step 6: Check if label column created successfully
print(df[['Accident_Severity', 'Accident_Severity_Label']].head())

# Step 7: Plot: Accident Severity Distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x="Accident_Severity_Label", order=["Slight", "Serious", "Fatal"])
plt.title("Accident Severity Distribution")
plt.ylabel("Number of Accidents")
plt.xlabel("Severity")
plt.show()

print(df['Accident_Severity'].unique())

# Show all column names
print(df.columns.tolist())

# Step 1: Load Dataset
df = pd.read_csv("/content/AccidentLondonBoroughs2223.csv")

# Step 2: Metadata Mappings (a few examples for now)
accident_severity_map = {1: 'Fatal', 2: 'Serious', 3: 'Slight'}
day_of_week_map = {1: 'Sunday', 2: 'Monday', 3: 'Tuesday', 4: 'Wednesday', 5: 'Thursday', 6: 'Friday', 7: 'Saturday'}
urban_rural_map = {1: 'Urban', 2: 'Rural', 3: 'Unallocated'}
light_conditions_map = {
    1: 'Daylight', 4: 'Dark - lights lit', 5: 'Dark - lights unlit',
    6: 'Dark - no lighting', 7: 'Dark - unknown'
}
weather_map = {
    1: 'Fine no wind', 2: 'Raining no wind', 3: 'Snow no wind',
    4: 'Fine + wind', 5: 'Rain + wind', 6: 'Snow + wind',
    7: 'Fog', 8: 'Other', 9: 'Unknown'
}

# Step 3: Apply Mappings
df['Accident_Severity_Label'] = df['Accident_Severity'].map(accident_severity_map)
df['Day_of_Week_Label'] = df['Day_of_Week'].map(day_of_week_map)
df['Urban_Rural_Label'] = df['Urban_or_Rural_Area'].map(urban_rural_map)
df['Light_Conditions_Label'] = df['Light_Conditions'].map(light_conditions_map)
df['Weather_Conditions_Label'] = df['Weather_Conditions'].map(weather_map)

# Step 4: Problem 1 â€“ Severity vs Weather & Light (EDA)
plt.figure(figsize=(10, 4))
sns.countplot(data=df, x='Light_Conditions_Label', hue='Accident_Severity_Label')
plt.title("Accident Severity vs Light Conditions")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 4))
sns.countplot(data=df, x='Weather_Conditions_Label', hue='Accident_Severity_Label')
plt.title("Accident Severity vs Weather Conditions")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Step 5: Problem 2 â€“ Urban vs Rural Area
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Urban_Rural_Label', hue='Accident_Severity_Label')
plt.title("Urban vs Rural Accident Severity")
plt.tight_layout()
plt.show()

# Step 6: Problem 3 â€“ Accidents by Day and Time
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='Day_of_Week_Label', order=[
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
])
plt.title("Accidents by Day of Week")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Optional: Convert Time to hour for further analysis
df['Hour'] = pd.to_datetime(df['Time'], errors='coerce').dt.hour

plt.figure(figsize=(10, 4))
sns.histplot(data=df, x='Hour', bins=24, kde=False)
plt.title("Accidents by Hour of the Day")
plt.xlabel("Hour (0-23)")
plt.tight_layout()
plt.show()

# Step 2: Define correct data types based on metadata
categorical_cols = [
    'Police_Force', 'Accident_Severity', 'Day_of_Week', 'Local_Authority_District',
    'Local_Authority_Highway', '1st_Road_Class', 'Road_Type', 'Junction_Detail', 'Junction_Control',
    '2nd_Road_Class', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities',
    'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions',
    'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area',
    'Did_Police_Officer_Attend_Scene_of_Accident'
]

numeric_cols = [
    'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude',
    'Number_of_Vehicles', 'Number_of_Casualties', '1st_Road_Number', '2nd_Road_Number', 'Speed_limit'
]

string_cols = ['Accident_Index', 'LSOA_of_Accident_Location']

datetime_cols = ['Date', 'Time']

# Step 3: Apply correct data types

# Categorical
for col in categorical_cols:
    df[col] = df[col].astype("category")

# Numeric
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# String
for col in string_cols:
    df[col] = df[col].astype(str)

# Date/Time
df['Date'] = pd.to_datetime(df['Date'], format="%d/%m/%Y", errors='coerce')
df['Time'] = pd.to_datetime(df['Time'], format="%H:%M", errors='coerce').dt.time

# Step 4: Verify data types
print("\n Final Column Data Types:")
print(df.dtypes)

from scipy.stats import skew, kurtosis
# For better plots
sns.set(style="whitegrid")

# Step 1: Dataset Shape
print("Total Records:", df.shape[0])
print("Total Variables (Columns):", df.shape[1])

# Step 2: Summary for Numeric Variables
numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()

summary_stats = df[numeric_cols].describe().T
summary_stats["mode"] = df[numeric_cols].mode().iloc[0]
summary_stats["skewness"] = df[numeric_cols].skew()
summary_stats["kurtosis"] = df[numeric_cols].kurtosis()
print("\nNumeric Variable Summary:\n")
print(summary_stats[["min", "max", "mean", "std", "mode", "skewness", "kurtosis"]])

# Step 3: Summary for Categorical Variables
categorical_cols = df.select_dtypes(include="category").columns.tolist()

print("\n Categorical Variable Summary:\n")
for col in categorical_cols:
    print(f" {col}:")
    print("  Unique Values:", df[col].nunique())
    print("  Mode:", df[col].mode().values[0])
    print("  Value Counts:\n", df[col].value_counts(), "\n")

import matplotlib.pyplot as plt
import seaborn as sns

# Numeric columns to analyze
num_plot_cols = ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # 1 row, 3 columns

# Set common style
sns.set_style("whitegrid")

# Histogram 1: Number of Vehicles
sns.histplot(df['Number_of_Vehicles'], bins=10, kde=True, ax=axes[0], color='steelblue')
axes[0].set_title(" Number of Vehicles in Accidents")
axes[0].set_xlabel("Number of Vehicles")
axes[0].set_ylabel("Frequency")
axes[0].text(1.5, 1000, "Most accidents involve 1-2 vehicles", fontsize=10, color='darkgreen')

# Histogram 2: Number of Casualties
sns.histplot(df['Number_of_Casualties'], bins=10, kde=True, ax=axes[1], color='tomato')
axes[1].set_title(" Number of Casualties in Accidents")
axes[1].set_xlabel("Number of Casualties")
axes[1].set_ylabel("Frequency")
axes[1].text(1.5, 1500, "Majority accidents cause 1 casualty", fontsize=10, color='darkred')

# Histogram 3: Speed Limit
sns.histplot(df['Speed_limit'], bins=10, kde=False, ax=axes[2], color='goldenrod')
axes[2].set_title(" Speed Limit Distribution at Accident Sites")
axes[2].set_xlabel("Speed Limit (mph)")
axes[2].set_ylabel("Frequency")
axes[2].text(30, 1600, "Most accidents occur at 30 mph", fontsize=10, color='brown')

# Adjust layout
plt.suptitle("ğŸ“Š Distribution of Key Accident Features", fontsize=18, fontweight='bold')
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# Custom label mapping based on metadata
severity_labels = {1: 'Fatal', 2: 'Serious', 3: 'Slight'}

# Map numeric codes to labels in a new column
df['Accident_Severity_Label'] = df['Accident_Severity'].map(severity_labels)

# Plot using labeled column
plt.figure(figsize=(7, 5))
sns.countplot(data=df, x='Accident_Severity_Label',
              order=['Slight', 'Serious', 'Fatal'],
              palette=['skyblue', 'orange', 'red'])

# Set titles and labels
plt.title("Distribution of Accident Severity Levels", fontsize=15, fontweight='bold')
plt.xlabel("Accident Severity")
plt.ylabel("Number of Accidents")
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Step 1: Weather code to label mapping based on metadata
weather_labels = {
    1: "Fine (No High Winds)",
    2: "Raining (No High Winds)",
    3: "Snowing (No High Winds)",
    4: "Fine + High Winds",
    5: "Raining + High Winds",
    6: "Snowing + High Winds",
    7: "Fog or Mist",
    8: "Other",
    9: "Unknown",
    -1: "Data Missing"
}

# Step 2: Map the Weather_Conditions column to human-readable labels
df['Weather_Condition_Label'] = df['Weather_Conditions'].map(weather_labels)

# Step 3: Plot using the new labeled column
plt.figure(figsize=(10, 5))
sns.countplot(data=df,
              x="Weather_Condition_Label",
              order=df['Weather_Condition_Label'].value_counts().index,
              palette="Blues")

# Step 4: Final touches
plt.title(" Weather Conditions During Road Accidents", fontsize=15, fontweight='bold')
plt.xlabel("Weather Condition", fontsize=12)
plt.ylabel("Number of Accidents", fontsize=12)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Count of missing values per column
missing_counts = df.isnull().sum()
missing_percent = (missing_counts / len(df)) * 100

missing_df = pd.DataFrame({
    "Missing Count": missing_counts,
    "Missing %": missing_percent.round(2)
}).sort_values(by="Missing %", ascending=False)

print(" Missing Value Summary:")
display(missing_df[missing_df["Missing Count"] > 0])

from scipy.stats import zscore

# Step 1: Define numeric columns you want to check
num_cols = ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']

# Step 2: Compute Z-scores
z_scores = df[num_cols].apply(zscore)

# Step 3: Identify outliers using Z-score threshold (> 3 or < -3)
outliers_z = (np.abs(z_scores) > 3)

# Step 4: Count outliers in each column
print("Number of outliers detected in each column (Z-score method):")
print(outliers_z.sum())

# Step 5: Optionally, show rows where outliers exist
df_outliers = df[outliers_z.any(axis=1)]
print("\n Sample rows with outliers:")
print(df_outliers[num_cols].head())



plt.figure(figsize=(14, 4))
for i, col in enumerate(num_cols):
    plt.subplot(1, 3, i+1)
    sns.boxplot(x=df[col], color="skyblue")
    plt.axvline(df[col].quantile(0.25), color='green', linestyle='--', label='Q1')
    plt.axvline(df[col].quantile(0.75), color='red', linestyle='--', label='Q3')
    plt.title(f"{col} Distribution with IQR")
    plt.legend()
plt.tight_layout()
plt.show()

def cap_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    data[column] = np.where(data[column] > upper, upper,
                            np.where(data[column] < lower, lower, data[column]))
    return data

# Apply capping to all relevant columns
for col in ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']:
    df = cap_outliers(df, col)

print(" Outliers capped using IQR limits.")

# Summary of extreme values
extremes = df[num_cols].describe().T[['min', 'max']]
print(" Extreme Value Summary:")
display(extremes)

# Define categorical columns for imbalance check
cat_cols = ['Accident_Severity', 'Day_of_Week', 'Light_Conditions',
            'Weather_Conditions', 'Urban_or_Rural_Area']

# Check value counts
for col in cat_cols:
    print(f"\n Value Distribution in '{col}':")
    print(df[col].value_counts(normalize=True).round(3) * 100)

# Check range consistency across 'Speed_limit'
print(" Unique Speed Limits:")
print(sorted(df['Speed_limit'].unique()))

# Import necessary libraries
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/AccidentLondonBoroughs2223.csv')

# STEP 1: Check Dataset Size
print(" Total Records (Instances):", df.shape[0])
print(" Total Features (Columns):", df.shape[1])

# STEP 2: Check for missing values
print("\nMissing Values Per Column:")
print(df.isnull().sum())

# STEP 3: Check for necessary columns for each business problem
required_cols = {
    "Problem 1 (Road & Weather Risk)": ['Accident_Severity', 'Road_Type', 'Weather_Conditions'],
    "Problem 2 (Factors Affecting Severity)": ['Accident_Severity', 'Speed_limit', 'Junction_Control', 'Light_Conditions', 'Urban_or_Rural_Area'],
    "Problem 3 (Prediction Model)": ['Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Road_Surface_Conditions', 'Weather_Conditions', 'Police_Force']
}

print("\n Column Check for Business Problems:")
for problem, cols in required_cols.items():
    missing_cols = [col for col in cols if col not in df.columns]
    if missing_cols:
        print(f" {problem}: Missing columns {missing_cols}")
    else:
        print(f" {problem}: All required columns are present")

# STEP 4: Final Conclusion
print("\n Final Conclusion:")
if all(col in df.columns for col_list in required_cols.values() for col in col_list):
    print(" The dataset is APPROPRIATE and SUFFICIENT to address all 3 business problems.")
else:
    print(" Some key columns are missing. Consider revising business problems.")

# Dictionary of variables to be used per business problem
analysis_tasks = {
    "Problem 1 - Impact of Road and Weather on Severity": {
        "Type": "Descriptive",
        "Variables": ['Accident_Severity', 'Road_Type', 'Weather_Conditions']
    },
    "Problem 2 - Key Factors in Urban vs Rural Accidents": {
        "Type": "Predictive",
        "Target": 'Accident_Severity',
        "Features": [
            'Speed_limit', 'Junction_Control', 'Light_Conditions',
            'Urban_or_Rural_Area', 'Road_Surface_Conditions',
            'Number_of_Vehicles', 'Number_of_Casualties'
        ]
    },
    "Problem 3 - Predicting Accident Severity": {
        "Type": "Predictive",
        "Target": 'Accident_Severity',
        "Features": [
            'Weather_Conditions', 'Road_Type', 'Road_Surface_Conditions',
            'Light_Conditions', 'Did_Police_Officer_Attend_Scene_of_Accident',
            'Urban_or_Rural_Area'
        ]
    }
}

# Display variable mapping clearly
print(" Variable Assignment per Analysis Task:\n")
for problem, detail in analysis_tasks.items():
    print(f" {problem} ({detail['Type']} Analysis):")
    if detail["Type"] == "Descriptive":
        print(f"    Variables: {', '.join(detail['Variables'])}")
    else:
        print(f"    Target Variable: {detail['Target']}")
        print(f"    Feature Variables: {', '.join(detail['Features'])}")
    print()

from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import pandas as pd

#  Step 1: Select Features for K-Means
features = ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']

# Make sure these columns exist and have no missing values
df_kmeans = df[features].dropna()

#  Step 2: Train-test split (for later use in predictive modeling too)
X = df_kmeans
y = df['Accident_Severity']  # Target for predictive modeling

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#  Step 3: K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_train)

# Add cluster labels to training set
X_train_with_clusters = X_train.copy()
X_train_with_clusters['Cluster'] = kmeans_labels

#  Optional: Show cluster centers
print(" Cluster centers:")
print(pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns))

#  1. Descriptive Modelling â€“ K-Means Clustering
kmeans_features = X_train.copy()  # Use training features only
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(kmeans_features)

# Attach cluster labels for analysis
X_train_clustered = kmeans_features.copy()
X_train_clustered['Cluster'] = kmeans_labels

# PCA for visualization (2D)
pca = PCA(n_components=2)
components = pca.fit_transform(kmeans_features)

plt.figure(figsize=(8, 5))
plt.scatter(components[:, 0], components[:, 1], c=kmeans_labels, cmap='viridis')
plt.title(" K-Means Clusters (PCA-reduced features)")
plt.xlabel("PC 1")
plt.ylabel("PC 2")
plt.grid(True)
plt.show()

# Silhouette score for quality of clustering
sil_score = silhouette_score(kmeans_features, kmeans_labels)
print(f" Silhouette Score: {sil_score:.2f}")

#  2. Descriptive Modelling â€“ Correlation Heatmap
plt.figure(figsize=(12, 8))
corr_matrix = X_train.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title(" Feature Correlation Heatmap")
plt.tight_layout()
plt.show()

#  3. Predictive Modelling â€“ Decision Tree Classifier
tree_clf = DecisionTreeClassifier(max_depth=5, random_state=42)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)

print(" Decision Tree Classification Report:")
print(classification_report(y_test, y_pred_tree))
print("Accuracy:", accuracy_score(y_test, y_pred_tree))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_tree))

#  4. Predictive Modelling â€“ K-Nearest Neighbors
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_clf.fit(X_train, y_train)
y_pred_knn = knn_clf.predict(X_test)

print(" KNN Classification Report:")
print(classification_report(y_test, y_pred_knn))
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import pandas as pd

# Step 1: Load the data
df = pd.read_csv("/content/AccidentLondonBoroughs2223.csv")

# Step 2: Select numeric features for clustering
features = ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']
df_cluster = df[features].dropna().sample(n=1000, random_state=42)  # Subset for speed

# Step 3: Silhouette Score for k=2 to 20
silhouette_scores = []
K_range = range(2, 21)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(df_cluster)
    score = silhouette_score(df_cluster, labels)
    silhouette_scores.append(score)

# Step 4: Plot silhouette scores
plt.figure(figsize=(10, 5))
plt.plot(K_range, silhouette_scores, marker='o', linestyle='--', color='green')
plt.title("ğŸ“Š Silhouette Scores for k=2 to 20 (K-Means Clustering)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

#  Target variable
y = df['Accident_Severity']
X = df[features].dropna()
y = y.loc[X.index]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#  Decision Tree: Grid Search
tree_params = {'max_depth': [3, 5, 10, None], 'criterion': ['gini', 'entropy']}
tree_model = GridSearchCV(DecisionTreeClassifier(random_state=42), tree_params, cv=5)
tree_model.fit(X_train, y_train)
print(" Best Decision Tree Params:", tree_model.best_params_)
y_pred_tree = tree_model.predict(X_test)
print(" Decision Tree Report:\n", classification_report(y_test, y_pred_tree))

#  KNN: Grid Search
knn_params = {'n_neighbors': [3, 5, 7, 9]}
knn_model = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
knn_model.fit(X_train, y_train)
print(" Best KNN Params:", knn_model.best_params_)
y_pred_knn = knn_model.predict(X_test)
print("KNN Report:\n", classification_report(y_test, y_pred_knn))

from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt

# Fit final model with best K (e.g., 3)
best_k = 3
kmeans = KMeans(n_clusters=best_k, random_state=42)
df_cluster['Cluster'] = kmeans.fit_predict(df_cluster)

# Cluster centers
print("Cluster Centers:")
print(pd.DataFrame(kmeans.cluster_centers_, columns=features))

# Step 2: Select and scale features
features = ['Number_of_Vehicles', 'Number_of_Casualties', 'Speed_limit']
df_cluster = df[features].dropna()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster)

# Step 3: KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X_scaled)

# Step 4: Create labeled DataFrame for plotting
X_train_clustered = pd.DataFrame(X_scaled, columns=features)
X_train_clustered['Cluster'] = labels

# Step 5: Pairplot to visualize clusters
sns.pairplot(X_train_clustered, hue='Cluster', diag_kind='kde', palette='Set2')
plt.suptitle("Cluster-wise Relationship Between Features", y=1.02)
plt.show()

corr = df[features].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title(" Correlation Among Numeric Variables")
plt.show()

severity_map = {1: 'Fatal', 2: 'Serious', 3: 'Slight'}
df['Accident_Severity_Label'] = df['Accident_Severity'].map(severity_map)

sns.countplot(data=df, x="Accident_Severity_Label", order=['Slight', 'Serious', 'Fatal'])
plt.title(" Accident Severity Distribution")
plt.ylabel("Number of Accidents")
plt.xlabel("Severity Level")
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

# Assuming X_train, X_test, y_train, y_test already prepared
#  Decision Tree
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

#  KNN
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

#  Evaluation
print(" Decision Tree Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))

print("\n KNN Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_knn))